{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Introducción al Uso de Modelos de Lenguaje de Última Generación (LLMs)\n",
        "\n",
        "Los modelos de lenguaje de última generación, conocidos como LLMs (Large Language Models), están revolucionando la forma en que las máquinas entienden y generan texto. Desde asistentes virtuales que responden a preguntas hasta sistemas avanzados de traducción automática, los LLMs se están utilizando en una amplia variedad de aplicaciones. Sin embargo, el uso y comprensión de estos modelos puede parecer un desafío, especialmente para quienes no tienen experiencia previa en este campo.\n",
        "\n",
        "En este curso, te guiaremos paso a paso a través del mundo de los LLMs. Nuestro objetivo es proporcionar una comprensión sólida y práctica de cómo se pueden utilizar estos modelos, incluso si partes desde cero. Aprenderás a:\n",
        "\n",
        "Comprender los fundamentos de los LLMs: Descubriremos cómo funcionan estos modelos, qué los hace tan poderosos y por qué son una herramienta esencial en el procesamiento de lenguaje natural (NLP).\n",
        "\n",
        "Trabajar con datasets: Te mostraremos cómo seleccionar, preparar y utilizar conjuntos de datos (datasets) para entrenar y evaluar modelos de lenguaje.\n",
        "\n",
        "Importar y utilizar modelos preentrenados: Aprenderás a utilizar modelos de Hugging Face, una de las bibliotecas más populares para trabajar con LLMs. Estos modelos preentrenados son un excelente punto de partida para muchas aplicaciones.\n",
        "\n",
        "Preentrenamiento y ajuste fino (fine-tuning): Te enseñaremos cómo realizar preentrenamiento y ajuste fino de un modelo, adaptándolo a tareas específicas utilizando tus propios datos.\n",
        "\n",
        "Aplicaciones prácticas: Finalmente, exploraremos cómo aplicar estos modelos en situaciones reales, desde la generación automática de texto hasta la creación de chatbots y más.\n",
        "\n",
        "Este curso está diseñado para ser accesible, práctico y completo, brindándote las habilidades necesarias para comenzar a utilizar LLMs en tus propios proyectos. ¡Empecemos este emocionante viaje hacia el futuro del procesamiento de lenguaje natural!"
      ],
      "metadata": {
        "id": "XPx5RJe7AsyM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Plataformas y Herramientas\n",
        "Hugging Face: Plataforma que facilita el acceso a modelos preentrenados y herramientas de procesamiento de lenguaje.\n",
        "Google Colab: Entorno gratuito de notebooks donde es fácil ejecutar código relacionado con LLMs.\n",
        "APIs: Interfaz para interactuar con modelos alojados externamente, como OpenAI o Hugging Face.\n",
        "Weights and Biases: Plataforma para seguimiento de experimentos y visualización de resultados de entrenamiento y ajuste fino.\n",
        "2. Modelos Abiertos\n",
        "GPT-3, BERT, T5: Modelos preentrenados que se pueden utilizar y ajustar para tareas específicas.\n",
        "LLaMA, Bloom: Modelos abiertos que permiten mayor control y personalización sin las restricciones de APIs propietarias.\n",
        "3. Tokenización\n",
        "Byte-Pair Encoding (BPE): Técnica para dividir el texto en subpalabras o tokens.\n",
        "WordPiece: Otra técnica de tokenización utilizada en modelos como BERT.\n",
        "Multilingual Tokenization: Consideraciones para manejar diferentes idiomas y caracteres.\n",
        "4. Quantización\n",
        "Quantización Post-entrenamiento: Reducir la precisión de los pesos del modelo para mejorar la eficiencia sin volver a entrenar.\n",
        "Quantización en entrenamiento: Ajuste del modelo durante el entrenamiento para optimizar tamaño y eficiencia.\n",
        "5. Fine-Tuning (Ajuste fino)\n",
        "Tuning en datos específicos: Ajustar un modelo preentrenado en un conjunto de datos específico para mejorar el rendimiento en tareas concretas.\n",
        "Transfer Learning: Aplicar conocimientos de un modelo preentrenado a una nueva tarea con menos datos de entrenamiento.\n",
        "Métodos de ajuste fino eficientes (LoRA, PEFT): Técnicas que permiten ajustar solo partes del modelo, reduciendo el costo computacional.\n",
        "6. Evaluación y Benchmarks\n",
        "Perplexity: Métrica que mide la incertidumbre del modelo en sus predicciones.\n",
        "BLEU/ROUGE: Métricas de evaluación para tareas de generación de texto como la traducción automática.\n",
        "GLUE, SuperGLUE: Conjuntos de evaluación estandarizados para medir el rendimiento en varias tareas NLP.\n",
        "7. Hiperparámetros\n",
        "Tasa de aprendizaje (Learning Rate): Parámetro clave para el ajuste fino de modelos.\n",
        "Batch Size: Tamaño de los lotes de datos que se utilizan durante el entrenamiento.\n",
        "Número de capas, número de cabezas de atención: Ajustes en la arquitectura del modelo para optimizar el rendimiento.\n",
        "8. Visualizaciones\n",
        "Embeddings: Representaciones visuales de cómo los modelos entienden el significado de las palabras.\n",
        "Atención: Visualizar las distribuciones de atención en modelos como Transformers para comprender cómo procesan la información.\n",
        "Gráficas de Entrenamiento: Monitoreo de la pérdida y precisión a lo largo del tiempo para ajustar el modelo.\n",
        "9. Uso de APIs\n",
        "OpenAI API, Hugging Face Inference API: Plataformas que permiten la utilización de modelos grandes sin necesidad de entrenarlos o ajustarlos localmente.\n",
        "Limitaciones y costos: Consideraciones sobre las cuotas y costos asociados con el uso de APIs."
      ],
      "metadata": {
        "id": "bTkjtTXWBrUE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PRIMER colab: Importar todo, 2do colab: pre-training-eval, 3er colab: Fine-tuning"
      ],
      "metadata": {
        "id": "N6jLxL8VCQ48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ¿Qué es un Modelo de Lenguaje y un Transformer?\n",
        "\n",
        "## Modelos de Lenguaje\n",
        "\n",
        "Un modelo de lenguaje es un tipo de algoritmo de inteligencia artificial diseñado para entender, generar y predecir texto basado en datos previos. Estos modelos son entrenados con grandes cantidades de texto, aprendiendo patrones en el uso del lenguaje, como la gramática, el significado de las palabras y la estructura de las frases. El objetivo principal de un modelo de lenguaje es predecir la siguiente palabra o secuencia de palabras en un texto, lo que le permite generar respuestas coherentes y contextualmente adecuadas.\n",
        "\n",
        "Los modelos de lenguaje se utilizan en una amplia variedad de aplicaciones, como la traducción automática, la generación de texto, los asistentes virtuales y el análisis de sentimientos. Los avances en el desarrollo de estos modelos han dado lugar a sistemas altamente sofisticados, como los Modelos de Lenguaje de Última Generación (LLMs) de los que hablaremos en este proyecto.\n",
        "\n",
        "## Transformers\n",
        "\n",
        "El Transformer es una arquitectura de red neuronal introducida en 2017 que ha revolucionado el campo del procesamiento de lenguaje natural (NLP). Antes de los Transformers, las redes neuronales recurrentes (RNNs) y las redes neuronales convolucionales (CNNs) eran las técnicas más comunes para trabajar con texto. Sin embargo, estas técnicas tenían limitaciones en términos de procesamiento paralelo y manejo de dependencias a largo plazo en las secuencias de texto.\n",
        "\n",
        "Los Transformers superaron estas limitaciones al introducir una técnica llamada \"atención\". El mecanismo de atención permite que el modelo se enfoque en diferentes partes de la secuencia de entrada para entender mejor el contexto de cada palabra. Esto significa que el modelo puede \"prestar atención\" a todas las palabras de una oración al mismo tiempo, en lugar de procesarlas una por una. Como resultado, los Transformers son más eficientes y efectivos para tareas como la traducción de idiomas, la generación de texto y el resumen de documentos.\n",
        "\n",
        "Una de las características más importantes de los Transformers es su capacidad para escalar. Los modelos basados en Transformers, como GPT-2 y GPT-3, contienen miles de millones de parámetros, lo que les permite capturar complejidades y matices del lenguaje humano de manera impresionante. Estos modelos han establecido nuevos estándares en la generación de texto, la comprensión de lenguaje y muchas otras aplicaciones de NLP.\n",
        "\n",
        "En resumen, los modelos de lenguaje y los Transformers representan avances significativos en la forma en que las máquinas pueden comprender y generar texto. Los Transformers, en particular, han abierto nuevas posibilidades para el desarrollo de sistemas de inteligencia artificial más potentes y efectivos en el procesamiento del lenguaje natural."
      ],
      "metadata": {
        "id": "W8MeWkG1GQDL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importamos un modelo"
      ],
      "metadata": {
        "id": "mpvVXlb6FkX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from IPython.display import display, HTML\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "BASE_MODEL = 'gpt2'  # More models at https://huggingface.co/models\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
        "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL)"
      ],
      "metadata": {
        "id": "UBfknCvlIGg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Hellow, uwu\"\n",
        "\n",
        "model_inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
        "\n",
        "# Generar una respuesta basada en la entrada\n",
        "generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
        "\n",
        "# Decodificar y mostrar la respuesta generada\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_W1Xw0oLVai",
        "outputId": "ca0fe7a2-26b2-4a5f-a45d-283cf9ee9709"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hellow, uwu, iwu, uwu, uwu, uwu, uwu, uwu, uwu, uwu, uwu, uwu, uwu, uwu, uwu, uwu, uwr - iu, li, juu, nuh, koo, ui, ni, sii-nuh ruh uuh ruuuu, unn, uuun, nuuuu, hui-nuh uuuun, nuh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "text_gen = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=\"gpt2\",\n",
        "    pad_token_id=50256\n",
        ")\n",
        "\n",
        "ejemplo=\"My name is Fabio Santiago. \"\n",
        "text_gen(ejemplo)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRa4dVgQNA4a",
        "outputId": "a556da3b-0c2c-4464-e7a3-56879ee530b6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': \"My name is Fabio Santiago. \\xa0I don't like to get too personal…\\xa0 But it's kind of a sad thing to say because sometimes life on Earth is full of people who have been through some rough times (they are still in\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    }
  ]
}